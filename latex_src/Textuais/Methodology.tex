%% =============================
%%      IMPORTANTE
%% ESTE ARQUIVO DEVE ESTAR SALVO COMO
%%      UTF - 8
%% =============================

% ----------------------------------------------------------
% Este capítulo é parte integrante do arquivo mestre
% Relatorio_TCC_Mestrado_Base_VERSÃO_SUBVERSÃO_FHZ
% ----------------------------------------------------------


% ----------------------------------------------------------
\chapter{Methodology}
\label{chap:methodology}
% ----------------------------------------------------------
The flow reconstruction methodology concerns aligning intricate simulations with less detailed counterparts in a practical manner, enabling the recreation of detailed flow patterns based on simplified simulations. This process comprises two primary stages: the dimensionality reduction step and the regression step.

The aim of the dimensionality reduction step is to facilitate the subsequent regression process by substantially reducing the number of variables employed as both input and output.
aleviating the known and up to date \textit{curse of dimensionality} problem.

In the context of the regression step, various function approximators can be employed. Neural Networks are a favorable choice due to their longstanding presence in the field of machine learning and their capability to handle extensive datasets from simulations, even when confronted with a plethora of variables (constituting our inputs and outputs). Alternatively, Gaussian processes, particularly in the form of Kriging, present themselves as a viable option owing to their expedited computational efficiency and interpretability, facilitated by their closed-form formulation.

The present flow reconstrudtion methodology is a purely data-driven approach and should work seamsly with either experimental or numerical data. For the sake of reducing costs and time requirements, this work is limited to the data syntheticly generated using numerical solvers. The follwing sections will describe the steps involved in the data generation, pre-processing, and post-processing of the flow fields in order to obtain the reduced order surregate surrogate model.

The main objective of the flow reconstruction strategy is to build a surrogate model able of predicting high-fidelity flow fields by using low fidelity data. The low fidelity data can be either sparse measurments of experimental data or numerical data generated by using fast and inaccurate solvers, generally, this low fidelity solver disregard some dimensions, physics involved, or both.

% Numerical solvers often returns large filds with a huge number of deegre of freedom. This could be a problem for the training process of the surrogate models, this problem first described by  as the \textit{curse of dimensionality}.

% This surrogate model is data-driven and trained using a dataset of low- and high-fidelity simulations. As the number of features to be modeled can be high, we employ a dimensionality reduction strategy using Singular Value Decomposition (SVD) to compress the datasets of low- and high-fidelity simulations. The functional mapping between these two distinct spaces is performed using a regression strategy such as Kriging or Neural Networks.

% The flow reconstruction methodology concerns aligning intricate simulations with less detailed counterparts in a practical manner, enabling the recreation of detailed flow patterns based on simplified simulations. This process comprises two primary stages: the dimensionality reduction step and the regression step.

% The aim of the dimensionality reduction step is to facilitate the subsequent regression process by substantially reducing the number of variables employed as both input and output.

% In the context of the regression step, various function approximators can be employed. Neural Networks are a favorable choice due to their longstanding presence in the field of machine learning and their capability to handle extensive datasets from simulations, even when confronted with a plethora of variables (constituting our inputs and outputs). Alternatively, Gaussian processes, particularly in the form of Kriging, present themselves as a viable option owing to their expedited computational efficiency and interpretability, facilitated by their closed-form formulation.

\section{Data Driver Approach}

Since our method relies entirely on data, where the data comes from doesn't really mattet, it could be gathered through experiments or artificially created using any numerical method. In this study, we're using synthetic data because it makes it easier, quickier, and cheapier to control numerical experiments, especially it allow us to generate dataset with different sizes and also evalueate our methodoloty with respect to data availability.

Regardless of the situation, the first step is to gather two sets of data: one representing the low fidelity space, it is our input, denoted by $\mathbf{X}$ and the other represent the targed high fidelity data, denoted by $\mathbf{y}$. The low-fidelity data could be a simulation with a rough grid, shortened dimensions, or other simplifications, and it might only include global scalar variables like geometry parameters and averaged boundary conditions. In constrast, the high-fidelity dataset usually involves the results of multidimensional simulation, able to capture as much of the physics as possible.

\section{Defining Data Matrix}

\section{Dimensionality Reduction}

\subsection{Numerical/Aplied Linear Algebra}

Linaer algera can simply be estated as the mathematics of vector and matrix, as so it has a paramout importance in many fields, including machine learning.

Numerial Linear algebra study how to aply linear algera to perform fast and accurate calculations using computers. This to objectives encouter two main complications, machine precision and machine speed.

Multidimensional vectors can be used to represet a lot of different complicated things, like images, mearusrements, voice, data, text and so forth.

Matrix is a grid o number or a list of vectors.

The flow reconstruction technique is specifically devised for addressing fluid flow fields, where the number of degrees of freedom is intricately tied to the elements in a full-order numerical simulation. In 3D flow fields, this association can result in an exceedingly large number of degrees of freedom, often reaching millions or even billions. Even in the context of the simplified 2D numerical simulation presented in this study, the number of degrees of freedom is on the order of $10^4$. This numerical value signifies the variables that the surrogate model must encapsulate. Therefore, any reduction in this count not only reflects a judicious application of compressed data but also yields a more cost-effective model.

In addition to the computational efficiency gained by diminishing the computational burden required for modeling the flow field, the process of dimensionality reduction also enhances the model's fitting capabilities. This is owing to the substantial reduction in the number of coupled linear equations that need to be modeled. In essence, the streamlined approach not only contributes to computational economy but also augments the model's efficacy in representing the underlying fluid dynamics.

To enact data compression, the matrices that compile training snapshots for low-fidelity and high-fidelity simulations, denoted as $\mathbf{X}_{l}$ and $\mathbf{X}_h$ respectively, underwent compression utilizing the truncated Singular Value Decomposition (SVD) technique. In essence, the SVD factorization, expressed in equation \ref{eq:svd}, possesses the capability to precisely represent any matrix $\mathbf{X}$ if the number of elements in the diagonal of the square singular values matrix $\mathbf{\Sigma}$ equals the original rank of the matrix $\mathbf{X}$. 

\begin{equation}
    \mathbf{X} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^T   
\end{equation}

Data compression is then achieved by truncating the number of singular values, retaining only the most significant ones, those with higher absolute values arranged in decreasing order. This process serves to distill essential information from the matrices, offering an effective means of compression while maintaining fidelity to the underlying data structure.

\subsection{Truncated SVD}

The singular value decomposition (SVD) is one of the most relevant and widelty used matrix factorization algorithm in numerical linear algebria, with many application such as dimensionality reduction and approximate solution for linear system of equations. In the context of this work the SVD will be used to perform a low-rank approximation of a generic data matrix $\mathbf{X}$ containing $m$ samples with dimensiolaity $n$. Given a generic data matrix $\mathbf{X}$, with row dimension $m$ representing the samples count and the column dimension $n$ representing the dimension or degree of freedom of each sample, the inner dimension $r$, or  matrix \textit{rank},  If $\mathbf{X}$ is at full rank, its decomposition into $\mathbf{X}$ and $\mathbf{B}$ provides a unique solution to the equation.

\begin{equation}
    \mathbf{X}_{m \times n} = \mathbf{B}_{m \times r} \times \mathbf{C}_{r \times n}.
\end{equation}

In fact the matrix $\mathbf{X}$ can be stored efficiently if whe are able to find a low rank approximation, such that $k << r$. In this case storing both $\mathbf{B}$ and $\mathbf{C}$ requires $(m+n)k$ storage units, instead of $mn$ for the full rank $\mathbf{X}$. Analougously, a matrix-vector multiplication would require $(m+n)k$ operations insteady of $mn$. The factorization also reveals relevat properties of the data in matrix $\mathbf{X}$.

\begin{equation}
    \mathbf{X}_{m \times n} \approx \mathbf{B}_{m \times k} \times \mathbf{C}_{k \times n}.
\end{equation}

This approximatio is extremely usefful to tackle the \textit{curse of dimensionality} problem. The low-rank approximation can be obtained by factoring the matrix $\mathbf{X}$ into orthonormal left $\mathbf{U}$ and right $\mathbf{V}^\dagger$ singular vectors and an $r$-dimensional non-negative diagonal matrix $\mathbf{\Sigma}$, of singular values ordered in descending order.

\begin{equation}
    \mathbf{X}_{m \times n} = \mathbf{U}_{m \times k} \mathbf{\Sigma}_{k \times k} {\mathbf{V}^\dagger}_{k \times n}
\end{equation}

This task can be performed by forming a small matrix $\mathbf{B}$, such that

\begin{equation}
    \mathbf{B}_{m \times k} = \mathbf{Q}_{m \times k}^\dagger \mathbf{X}_{k \times k} 
\end{equation}

And the computing the singular value decomposition of $\mathbf{B}$

\begin{equation}
    \mathbf{B}_{m \times k} =  \mathbf{\tilde U}_{m \times k}^\dagger \mathbf{\Sigma}_{k \times k} \mathbf{V}_{k \times k}^\dagger
\end{equation}

Finally projecting 

\begin{equation}
    \mathbf{U}_{m \times n} = \mathbf{Q}_{m \times k}\mathbf{U}_{k \times k}
\end{equation}

The matrix $\mathbf{Q}$ is the orthonormal matrix that forms a basis for the range of $\mathbf{X}$, it can be efficiently computed using randomized algorithms to solve the problem of given $k < min(m,n)$, finding a $m \times k$ matrix such that

\begin{equation}
    \mathbf{X} \approx \mathbf{Q} \mathbf{Q}^\dagger \mathbf{X}
\end{equation}

Matrix $\mathbf{Q}$ can be computed directly by a QR factorization in the form of $\mathbf{X} \approx \mathbf{Q}\mathbf{R}\mathbf{P}$ and then setting $\mathbf{B} = \mathbf{P}^\dagger \mathbf{R}$. Instead, to solve it using a randomized sampling we start by building random matrix $\mathbf{\Omega}_{n \times k}$, suppose it to be Gaussian. Then pré-multiply it

\begin{equation}
    \mathbf{Y} = \mathbf{X} \mathbf{\Omega}
\end{equation}

Construct a orthonormal matrix $\mathbf{Q}$ such that

\begin{equation}
    \mathbf{Y} = \mathbf{Q} \mathbf{Q}^dagger \mathbf{X}
\end{equation}

the basis vector $\mathbf{Q}$ and its self adjoint or conjugate transpose matrix $\mathbf{Q}^{\dagger}$, satisfying

\begin{equation}
    \mathbf{X} \approx \mathbf{Q}^{\dagger} \mathbf{Q} \mathbf{X}
\end{equation}

A randomized algorithm is used to find $\mathbf{Q}$ tha minimizaes

\begin{equation}
    \norm{\mathbf{X} - \mathbf{Q}^{\dagger} \mathbf{Q} \mathbf{X}} 
\end{equation}

\begin{equation}
    \mathbf{B} = \mathbf{Q}^{\dagger} \times \mathbf{X}
\end{equation}

Continue to compile due to autosave. New entry. 

\section{POD}

POD, also known as principal component analysis (PCA), empirical orthogonal functions (EOF), the Hotelling transform, or a Karhunen-Loeve expansion in various domains, is a technique to reduce the dimensionality of complex data sets. This method is extensively used in numerical simulations and physical experiments to extract dominant patterns of variability from a high-dimensional dataset.

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../thesis_allan"
%%% End:
