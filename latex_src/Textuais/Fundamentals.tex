\chapter{Foundations of Reduced-Order Modeling and Machine Learning in Fluid Dynamics}
\label{chap:lit_review}

\section{ The Imperative for Model Reduction in Computational Aerodynamics}

The governing equations of fluid motion, the Navier-Stokes equations, are a set of coupled, nonlinear partial differential equations. Their numerical solution in a discretized domain representing a complex engineering geometry results in a system with an enormous number of degrees of freedom, often on the order of millions or billions. When considering parametric studies, where design variables such as geometry or boundary conditions are varied, the dimensionality of the problem space explodes further. This leads to the well-known "curse of dimensionality," where the number of samples required to adequately explore a design space grows exponentially with the number of parameters. A brute-force approach, involving the gridding of this high-dimensional parameter space and running a full-order simulation at each point, is computationally infeasible.

This reality underscores the fundamental imperative for model reduction. The challenge is not merely one of computational cost, but one of inherent complexity and high dimensionality of the solution manifold—the set of all possible solutions as the input parameters vary. The central hypothesis of reduced-order modeling is that despite the high dimensionality of the discretized system, the actual dynamics of many physical systems evolve on or near a much lower-dimensional, intrinsic manifold embedded within the high-dimensional state space. A successful ROM, therefore, is one that can discover and exploit this low-dimensional structure. The task of a ROM is not just to compress data, but to identify the underlying coherent patterns and structures that govern the system's behavior, thereby creating a compact, physically meaningful, and computationally tractable representation of the original complex system.

\section{Dimensionality Reduction for Fluid Flows: Proper Orthogonal Decomposition (POD)}

Proper Orthogonal Decomposition (POD) is arguably the most established and widely used technique for dimensionality reduction in fluid dynamics. Also known in other fields as Principal Component Analysis (PCA) or the Karhunen-Loève expansion, POD provides a systematic method for finding the most efficient linear basis to represent a high-dimensional dataset. 

\subsection{Theoretical Formulation}

The application of POD in CFD typically begins with the ``method of snapshots,'' a technique pioneered for fluid dynamics applications. A set of $N_s$ high-fidelity simulation results, or ``snapshots,'' are collected. Each snapshot, representing a flow field at a specific parameter value, is reshaped into a column vector $u_j \in \mathbb{R}^{N_g}$, where $N_g$ is the number of grid points in the mesh. These snapshot vectors are then assembled into a snapshot matrix $S=[u_1, u_2, ..., u_{N_s}] \in \mathbb{R}^{N_g \times N_s}$.

POD seeks a set of orthonormal basis vectors, or ``modes,'' $\{\phi_k\}_{k=1}^r$, that are optimal in the sense that they maximize the projection of the snapshot data onto them. Mathematically, this is equivalent to finding the basis that minimizes the mean squared error of the projection for any given rank of approximation. The solution to this optimization problem is found by solving the eigenvalue problem of the data covariance matrix, $C=SS^T$. However, since $N_g$ is typically much larger than $N_s$, it is computationally more efficient to solve the eigenvalue problem for the smaller matrix $K=S^TS \in \mathbb{R}^{N_s \times N_s}$. The POD modes $\phi_k$ are then recovered from the eigenvectors of $K$. In practice, this entire procedure is most robustly and efficiently performed using the Singular Value Decomposition (SVD) of the snapshot matrix, $S=U\Sigma V^T$, where the columns of the matrix $U$ are the POD modes $\phi_k$.

\subsection{The "Energy-Optimal" Basis}

A key property of POD is that the modes form an "energy-optimal" basis. The singular values $\sigma_k$ (the diagonal entries of $\Sigma$) are related to the eigenvalues of the covariance matrix and represent the "energy" (or variance) captured by each corresponding mode $\phi_k$. The modes are ordered hierarchically, such that the first mode $\phi_1$ captures the most energy, $\phi_2$ captures the most of the remaining energy, and so on. This hierarchy allows for a highly efficient low-rank approximation of the original data. Any snapshot $u_j$ can be approximated as a linear combination of the first $M$ modes:

$$u_j \approx \sum_{k=1}^{M} a_{jk} \phi_k$$

where $M \ll N_s$, and the coefficients $a_{jk} = u_j^T \phi_k$ are the projections of the snapshot onto the modes. By retaining only a small number of modes that capture a vast majority of the system's energy (e.g., 99.9\%), the dimensionality of the problem is dramatically reduced from $N_g$ to $M$.

\subsection{Implementation and Preprocessing}

Before applying POD, raw snapshot data must be preprocessed to ensure numerical stability and physical relevance. Two steps are crucial. First, the data is typically mean-centered by subtracting the ensemble-averaged field, $\bar{u} = \frac{1}{N_s}\sum_{j=1}^{N_s} u_j$, from each snapshot. POD is then performed on the fluctuation fields, $u_j' = u_j - \bar{u}$. This separates the mean behavior from the dynamic variations, which are often the primary interest of the model. Second, data scaling, for instance using a MinMaxScaler to bring all values into a uniform range like $[0, 1]$, is often applied. This prevents fields with large physical magnitudes (like pressure) from dominating the variance calculation over fields with smaller magnitudes (like temperature or Mach number), ensuring an equitable contribution from all physical quantities to the final modes.

\subsection{Limitations and Future Directions}

The power of POD lies in its simplicity and optimality for linear systems. However, its linearity is also its most fundamental weakness. POD finds the optimal linear subspace on which to project the data. Many important fluid dynamics phenomena, particularly those dominated by advection or featuring moving discontinuities like shock waves, are better described as evolving on a nonlinear manifold. Projecting a curved manifold onto a flat, linear subspace is an inherently inefficient representation. It often requires a large number of POD modes to accurately capture the nonlinear dynamics, diminishing the benefits of the model reduction. 

This limitation has been a major driver of recent research in the field. The recognition that POD is suboptimal for strongly nonlinear systems has motivated the exploration of nonlinear dimensionality reduction techniques. Chief among these are methods based on deep learning, such as Convolutional Autoencoders (CAEs). A CAE uses a neural network (the encoder) to learn a nonlinear mapping from the high-dimensional input space to a low-dimensional latent space, and another network (the decoder) to map back. By training the network to minimize reconstruction error, the CAE can learn a compact, nonlinear representation that is potentially far more efficient than the linear subspace found by POD. The development and application of such nonlinear ROMs represent the next frontier in data-driven fluid dynamics, promising even greater efficiency and accuracy for the most challenging flow problems. This thesis, while focused on the robust application of POD, acknowledges this trajectory and provides a foundational framework upon which such future advancements can be built. 

\section{Surrogate Modeling for Latent Space Dynamics}

Once dimensionality reduction has been performed, the original problem of mapping high-dimensional inputs to high-dimensional outputs is transformed into a more tractable one: mapping low-dimensional inputs (the design parameters) to a low-dimensional latent space (the POD coefficients). This is a classical regression problem, for which a variety of machine learning techniques can be employed. This work focuses on two of the most powerful and widely used methods: Gaussian Process Regression and Artificial Neural Networks.

\subsection{Gaussian Process Regression (GPR)}

Gaussian Process Regression is a non-parametric, Bayesian regression method that has gained significant popularity in engineering and machine learning. Unlike parametric models that fit a single function to the data, GPR defines a probability distribution over a space of functions. A Gaussian Process is a collection of random variables, any finite number of which have a joint Gaussian distribution. It is fully specified by a mean function and a covariance function, or kernel.

The kernel is the heart of a GPR model. It encodes the assumptions about the function being modeled, such as its smoothness and correlation structure. A common choice, used in this work, is the \textbf{Radial Basis Function (RBF) kernel}, also known as the squared exponential kernel. This kernel assumes that input points that are "close" in the parameter space will have similar output values, with the notion of "closeness" controlled by a length-scale hyperparameter.

The key advantages of GPR are twofold. First, as a Bayesian method, it provides not only a point prediction (the posterior mean) but also a measure of predictive uncertainty (the posterior variance). This is invaluable in engineering design, as it allows for principled uncertainty quantification and risk assessment. Second, the Bayesian framework provides a natural defense against overfitting, making GPR particularly robust for problems with small or sparse training datasets. However, GPR is not without its drawbacks. The primary limitation is its computational complexity, which scales as $O(N^3)$ with the number of training points $N$, due to the need to invert the covariance matrix. This makes standard GPR computationally challenging for very large datasets.

\subsection{Artificial Neural Networks (ANNs)}

Artificial Neural Networks, and specifically the Multi-Layer Perceptron (MLP), are powerful function approximators inspired by the structure of the biological brain. The universal approximation theorem states that a feedforward network with a single hidden layer containing a finite number of neurons can approximate any continuous function to an arbitrary degree of accuracy, given enough neurons. This makes ANNs an extremely flexible and powerful tool for regression.

An ANN consists of interconnected layers of nodes, or "neurons." Each neuron performs a weighted sum of its inputs, adds a bias, and then passes the result through a nonlinear activation function (e.g., sigmoid, ReLU, tanh). By stacking multiple layers, ANNs can learn hierarchical representations of data and model highly complex, nonlinear relationships.

The primary advantages of ANNs are their scalability to very large datasets and their unparalleled flexibility in modeling complex functions. Unlike GPR, their training time can scale more favorably with the number of samples, especially when using modern GPU hardware and stochastic gradient descent-based optimizers. However, this flexibility comes at a cost. ANNs are often considered "black-box" models, making their internal reasoning difficult to interpret. They are also prone to overfitting if not properly regularized (e.g., using techniques like dropout or weight decay), and they typically have a large number of hyperparameters (e.g., number of layers, number of neurons, learning rate) that require careful and often extensive tuning to achieve optimal performance. 

The choice between GPR and ANNs represents a fundamental trade-off in surrogate modeling. GPR offers probabilistic rigor, built-in uncertainty quantification, and robustness on small datasets, making it an excellent choice for well-posed problems where quantifying uncertainty is paramount. ANN, on the other hand, offers scalable flexibility and the raw power to model extremely complex, high-dimensional, and noisy phenomena, provided that sufficient data and computational resources are available for training and tuning. As demonstrated in Chapter 5 of this dissertation, the empirical evidence gathered from comparative studies is crucial for navigating this trade-off. The results suggest that there is no single "best" model; rather, the optimal choice depends on the specific characteristics of the problem at hand, including the size and quality of the available data and the nature of the underlying physics.

\section{Toward Physically Consistent and Interpretable Models}

As machine learning models become more integrated into safety-critical engineering workflows, two challenges have come to the forefront of the research community: physical consistency and interpretability. A model that is purely data-driven, with no knowledge of the underlying physics, may produce predictions that are highly accurate on average but violate fundamental physical laws, making them unreliable for engineering decisions. Similarly, a "black-box" model that provides accurate predictions without any explanation of its reasoning is difficult to trust, debug, or certify for use in critical applications.


\subsection{The Rise of Physics-Informed Machine Learning (PIML)}

To address the first challenge, the field of \textbf{Physics-Informed Machine Learning (PIML)} has emerged. The core idea of PIML is to embed physical domain knowledge, often in the form of governing partial differential equations (PDEs), directly into the machine learning algorithm. A common approach is to add the residuals of the governing PDEs as a penalty term in the model's loss function. This forces the model to learn not only from the data but also to satisfy the physical constraints, leading to better data efficiency, improved generalization, and more physically plausible solutions.

The work in this dissertation aligns with the PIML philosophy through a practical, "soft" implementation. The novel hybrid loss function introduced in Chapter 5 for training ANN surrogates includes a term, $L_{\text{reconstructed}}$, which directly measures the prediction error in the reconstructed physical space. By minimizing this term, the training process implicitly pushes the model to generate solutions that are physically accurate, as any significant deviation from the true physical field would incur a large penalty. This approach bridges the gap between the abstract latent space, where the model operates, and the physical space, which is the domain of interest for the engineer.


\section{The Interpretability Imperative}

To address the second challenge of trust, methods for model interpretability have become essential. It is no longer sufficient for a model to be accurate; it must also be explainable. Techniques like SHapley Additive exPlanations (SHAP) provide a powerful framework for peering inside the black box. Based on principles from cooperative game theory, SHAP assigns a quantitative "importance value" to each input feature, representing its contribution to a specific prediction. This allows one to understand which factors the model is weighing most heavily in its decision-making process. 

The application of SHAP analysis in Chapter 5 is a direct response to this interpretability imperative. By analyzing the feature importances of the trained surrogate models, it is possible to verify that their behavior aligns with physical intuition. For instance, the finding that the model identifies inlet total pressure as the most critical parameter for nozzle flow builds confidence that the model has learned a physically meaningful relationship, rather than just spurious correlations in the data. Therefore, this dissertation does not simply apply machine learning as a black-box tool; it actively engages with two of the most critical and contemporary conversations in the field of scientific machine learning: the pursuit of physical consistency and the establishment of trust through interpretability. 